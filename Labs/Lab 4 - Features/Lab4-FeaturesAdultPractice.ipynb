{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing practice with Adult data\n",
    "\n",
    "Now it is time to try ourselves so let us use the Adult data again to do some FC/FS, etc.  Reload libraries and data as we did in the cleaning lab.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "Now we begin by loading the data as we have done before and printing the `.head()` to inspect the data.  Load the data as the Adult_data data frame. We will use that through the lab. You have already explored this dataset in detail so you know already quite a bit about it, hopefully!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us examine what type of attributes may be to understand which are non numeric and may need encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now divide our columns in to the categorical and numerical columns. Make a list of categorical and numerical columns.  We may already discard 'fnlwgt' at this stage as it is not a useful variable for the prediction of income.  We can exclude it here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the categorical values, we can find the number of unique fields in each to understand the different types of encodings that we may apply.  If we have too many unique values then hot-encoding may not be right and we may use a `ordinal encoding` instead.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature construction\n",
    "\n",
    "One column that we could transform is _native-country_ as it has too many values to be useful.  Let us first visualise it against the decision or target variable _income_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that it has to many not particularly useful values.  We could convert this to a variable that has a value of 1 if the person has 'United_States' as 'native-country' and 0 for all others.  So we count those born in the USA against all others.  Transform the variable in this way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now visualise it again, against the target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perhaps combine _capital_gain_ and _capital-loss_ into a more useful variable.  Let us see what there values may look like against one another.  We can do this by creating a scatterplot to show _capital_gain_ against _capital-loss_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to show that when _capital-loss_ has a non zero value _capital-gain_ is zero and viceversa.  To make sure this is the case, let us select any records which have both  _capital_gain_ and _capital-loss_ as positive values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are no records, we could transform these two into a new variable, say _GainLoss_ which has a value of -1 if the person reported any _capital-loss_, 1, if they reported a _capital-gain_ and 0 of they reported neither.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualise values for the new variable against the _income_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical encoding\n",
    "So we have transformed or created a couple of variables.  What categorical variables are now left in the data frame? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose some of those to be encoded with an `OrdinalEncoder` and perform the encoding.  Put the encoded new variables along with the other data in a new data frame called `X`.  I will leave the smallest 3 in terms of list amount of unique values for `OneHotEncoding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at the contents of X so far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us perform `OneHotEncoding` for some other variables such as _gender_.  Append the new variables to _X_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us do similar `OneHotEncoding` encoding of _Race_ and append to`X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us also `OneHotEncode` the _relationship_ attribute and append also to `X`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "Let us look at all our numeric columns now present in `X`. We can select those as the columns we will use to now do some Feature Selection.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now make a new version of 'X', say 'X1', which contains the numeric columns we just selected.  Let us define our new `X1` which will contain all the numeric input features, and `y`, our oputput variable, which is still _income_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can call a feature ranking algorithm to rank all the features on `X1` using the `mutual_info_classif` criterion.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the feature names with their score in order of ranking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation of variables \n",
    "\n",
    "This may have difficulty medium as we have not covered it but it is for you to do some research on how to do new things to the data finding the relevant functions, methods, etc.  \n",
    "\n",
    "Just to finish, you could look also at the correlation between different variables.  Correlation can also sometimes be used to choose different variables (for example eliminating those that are highly correlated).  For this you will can call the correlation function from Pandas  https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualise the correlations by using a `heatmap` from seaborn https://seaborn.pydata.org/generated/seaborn.heatmap.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also select columns on the basis of the correlation calculations, for example we could select those where the correlation is greater than some threshold value (say 0.7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you leave the lab you may spend sometime thinking about how we will put all of what we have learned together.  For example, to now go and analyse the Adult data with different classification algorithms, what steps should you apply to the data and in what order?  Do you look at missing data? outliers? balancing? FC? FS? Sampling?  Which of those do you apply to the data before spliting into train and test set and which to the train set only? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
